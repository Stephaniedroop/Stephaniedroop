Hi, I’m Steph. 

I'm a PhD student at University of Edinburgh in computational cognitive modelling (aka 'doing psychology with the tools of computer science'). My research is on how we decide something is a cause of something else. I use the methodology of causal structural models and Bayesian inference, and work from simpler to richer scenarios, ending on modelling people's free text explanations for a variety of outcomes.

The main repos on here to see my work are from my PhD:

1. Collider (either '_cogsci25' or '_cognition'). Chapter 1 of my thesis: where I model how people combine causal inference and causal selection in a simple determinative structure where two possible causes A and B can cause effect E, and each has an accompanying unobserved counterpart to handle the uncertainty and so allow modelling more complex scenarios.
2. Gridworld ('gw'). Chapter 2 of my thesis: where I model outcome of a complex combination of causes using an extension of the structural causal model approach from the Collider project. The setup is agents with various different biographical and environmental details which make their behaviour more or less congruous. All behaviours (outcomes) are possible. Which fact of the situation will people cite as THE cause of the outcome? I model this as sometimes transient unobserved factors 'disrupt' stable causes.
3. Continuation of gridworld: modelling in process at Autumn 2025. Currently in Exp2/Model folder of gw.

Other good repos are `TuringToM` which was as standalone project on a Turing test for large language models. The data got largely superseded when the field moved very fast, but there are some good visualisations in ggplot. 


You can read more details on the README for each project.

Some repos are old, sells or just for practice. Avoid any that have not been updated since 2022! 

### Declaration on generative AI 
Sometimes I use generative AI to help write code, either in the form of GitHub copilot auto-complete for one-liners whose purpose I already specified in a comment, or I ask e.g. Perplexity for help debugging an error or debugging or writing a call to a package, etc.

I always test the result by progressively removing parts and noting how it behaves. For me in code, legibility, functionality and standardisation are more important than individuality. I want to write code that works and is usable. Through using genAI I have found that the way I was taught R in 2018 is no longer considered best practice, for example, and I want to prioritise production-ready over idiosyncrasy. 

I have never and will never use generative AI to write **text** instead of my own voice. The difference is that in writing, it is important that I speak as me personally. Our thoughts have been encoded in words for longer. 

<!---
Stephaniedroop/Stephaniedroop is a ✨ special ✨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
